<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Kernel-3.10.0-957.el7_v | oosTech.com</title>
  <meta name="description" content="Documentation for &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;*    kernel version 2.6.29    (c) 1998, 1999,  Rik van Riel &amp;#114;&amp;#x69;&amp;#101;&amp;#108;&amp;#64;&amp;#110;&amp;#108;&amp;#46;&amp;#108;&amp;#x69;&amp;#110;&amp;#x75;&amp;#120;&amp;#x2e;&amp;#x6f;&amp;#114;&amp;#x67;    (c) 2">
<meta property="og:type" content="article">
<meta property="og:title" content="Kernel-3.10.0-957.el7_v">
<meta property="og:url" content="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_vm/index.html">
<meta property="og:site_name" content="oosTech">
<meta property="og:description" content="Documentation for &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;*    kernel version 2.6.29    (c) 1998, 1999,  Rik van Riel &amp;#114;&amp;#x69;&amp;#101;&amp;#108;&amp;#64;&amp;#110;&amp;#108;&amp;#46;&amp;#108;&amp;#x69;&amp;#110;&amp;#x75;&amp;#120;&amp;#x2e;&amp;#x6f;&amp;#114;&amp;#x67;    (c) 2">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-03-12T16:00:00.000Z">
<meta property="article:modified_time" content="2021-03-12T16:00:00.000Z">
<meta property="article:author" content="Sam Lee">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_vm/index.html">
  
    <link rel="alternate" href="/atom.xml" title="oosTech" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img src="/images/avatar.png" width="400" height="400">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">oosTech by Sam Lee</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="站内搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">站点首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档文档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">文档分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">常用链接</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">共享白板</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://google.com/" target="_blank" title="Google" data-toggle=tooltip data-placement=top><i class="icon icon-google"></i></a></li>
        
        <li><a href="https://twitter.com/" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告牌</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p> 欢迎来到oosTech ，我是一个Linux 拥趸：D。 目前正在用的Linux 版本是 Red Hat Enterprise Linux release 8.3 </p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">文档分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-2-6-32-573-12-1-el6-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_2.6.32-573.12.1.el6_内核文档</a><span class="category-list-count">830</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a><span class="category-list-count">1658</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-4-18-0-80-el8-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_4.18.0-80.el8_内核文档</a><span class="category-list-count">3937</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_3270/" class="title">Kernel-3.10.0-957.el7_3270</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-2-6-32-573-12-1-el6-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_2.6.32-573.12.1.el6_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-2.6.32-573.12.1.el6_devices/" class="title">Kernel-2.6.32-573.12.1.el6_devices</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_3c509/" class="title">Kernel-3.10.0-957.el7_3c509</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_4CCs/" class="title">Kernel-3.10.0-957.el7_4CCs</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_53c700/" class="title">Kernel-3.10.0-957.el7_53c700</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-lowmem-reserve-ratio-is-an-array-You-can-see-them-by-reading-this-file"><span class="toc-number">1.</span> <span class="toc-text">The lowmem_reserve_ratio is an array. You can see them by reading this file.</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vfs-cache-pressure"><span class="toc-number">2.</span> <span class="toc-text">vfs_cache_pressure</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-Kernel-3.10.0-957.el7_vm" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Kernel-3.10.0-957.el7_v
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2021/03/13/Kernel-3.10.0-957.el7_vm/" class="article-date">
	 published: <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
	</a>
</span>

        
	<a href="/2021/03/13/Kernel-3.10.0-957.el7_vm/" class="article-date">
	   updated: <time datetime="2021-03-12T16:00:00.000Z" itemprop="dateUpdated">2021-03-13</time>
	</a>


        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
  </span>

        

        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill"></i>
	    <!--<span id="busuanzi_container_page_pv" style="display:inline;">-->
			<span id="busuanzi_value_page_pv" style="display:inline;"></span>
		<!--</span>-->
	</span>



        <!--<span class="post-comment"><i class="icon icon-comment"></i> <a href="/2021/03/13/Kernel-3.10.0-957.el7_vm/#comments" class="article-comment-link">评论</a></span> -->
        
	
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p>Documentation for /proc/sys/vm/*    kernel version 2.6.29<br>    (c) 1998, 1999,  Rik van Riel <a href="mailto:&#114;&#x69;&#101;&#108;&#64;&#110;&#108;&#46;&#108;&#x69;&#110;&#x75;&#120;&#x2e;&#x6f;&#114;&#x67;">&#114;&#x69;&#101;&#108;&#64;&#110;&#108;&#46;&#108;&#x69;&#110;&#x75;&#120;&#x2e;&#x6f;&#114;&#x67;</a><br>    (c) 2008         Peter W. Morreale <a href="mailto:&#112;&#109;&#x6f;&#x72;&#114;&#101;&#x61;&#108;&#x65;&#64;&#110;&#x6f;&#118;&#x65;&#108;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;">&#112;&#109;&#x6f;&#x72;&#114;&#101;&#x61;&#108;&#x65;&#64;&#110;&#x6f;&#118;&#x65;&#108;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a></p>
<p>For general info and legal blurb, please look in README.</p>
<p>==============================================================</p>
<p>This file contains the documentation for the sysctl files in<br>/proc/sys/vm and is valid for Linux kernel version 2.6.29.</p>
<p>The files in this directory can be used to tune the operation<br>of the virtual memory (VM) subsystem of the Linux kernel and<br>the writeout of dirty data to disk.</p>
<p>Default values and initialization routines for most of these<br>files can be found in mm/swap.c.</p>
<p>Currently, these files are in /proc/sys/vm:</p>
<ul>
<li>admin_reserve_kbytes</li>
<li>block_dump</li>
<li>compact_memory</li>
<li>dirty_background_bytes</li>
<li>dirty_background_ratio</li>
<li>dirty_bytes</li>
<li>dirty_expire_centisecs</li>
<li>dirty_ratio</li>
<li>dirty_writeback_centisecs</li>
<li>drop_caches</li>
<li>extfrag_threshold</li>
<li>hugepages_treat_as_movable</li>
<li>hugetlb_shm_group</li>
<li>laptop_mode</li>
<li>legacy_va_layout</li>
<li>lowmem_reserve_ratio</li>
<li>max_map_count</li>
<li>memory_failure_early_kill</li>
<li>memory_failure_recovery</li>
<li>min_free_kbytes</li>
<li>min_slab_ratio</li>
<li>min_unmapped_ratio</li>
<li>mmap_min_addr</li>
<li>mmap_rnd_bits</li>
<li>mmap_rnd_compat_bits</li>
<li>nr_hugepages</li>
<li>nr_overcommit_hugepages</li>
<li>nr_trim_pages         (only if CONFIG_MMU=n)</li>
<li>numa_zonelist_order</li>
<li>oom_dump_tasks</li>
<li>oom_kill_allocating_task</li>
<li>overcommit_kbytes</li>
<li>overcommit_memory</li>
<li>overcommit_ratio</li>
<li>page-cluster</li>
<li>panic_on_oom</li>
<li>percpu_pagelist_fraction</li>
<li>stat_interval</li>
<li>swappiness</li>
<li>user_reserve_kbytes</li>
<li>vfs_cache_pressure</li>
<li>zone_reclaim_mode</li>
</ul>
<p>==============================================================</p>
<p>admin_reserve_kbytes</p>
<p>The amount of free memory in the system that should be reserved for users<br>with the capability cap_sys_admin.</p>
<p>admin_reserve_kbytes defaults to min(3% of free pages, 8MB)</p>
<p>That should provide enough for the admin to log in and kill a process,<br>if necessary, under the default overcommit ‘guess’ mode.</p>
<p>Systems running under overcommit ‘never’ should increase this to account<br>for the full Virtual Memory Size of programs used to recover. Otherwise,<br>root may not be able to log in to recover the system.</p>
<p>How do you calculate a minimum useful reserve?</p>
<p>sshd or login + bash (or some other shell) + top (or ps, kill, etc.)</p>
<p>For overcommit ‘guess’, we can sum resident set sizes (RSS).<br>On x86_64 this is about 8MB.</p>
<p>For overcommit ‘never’, we can take the max of their virtual sizes (VSZ)<br>and add the sum of their RSS.<br>On x86_64 this is about 128MB.</p>
<p>Changing this takes effect whenever an application requests memory.</p>
<p>==============================================================</p>
<p>block_dump</p>
<p>block_dump enables block I/O debugging when set to a nonzero value. More<br>information on block I/O debugging is in Documentation/laptops/laptop-mode.txt.</p>
<p>==============================================================</p>
<p>compact_memory</p>
<p>Available only when CONFIG_COMPACTION is set. When 1 is written to the file,<br>all zones are compacted such that free memory is available in contiguous<br>blocks where possible. This can be important for example in the allocation of<br>huge pages although processes will also directly compact memory as required.</p>
<p>==============================================================</p>
<p>dirty_background_bytes</p>
<p>Contains the amount of dirty memory at which the background kernel<br>flusher threads will start writeback.</p>
<p>Note: dirty_background_bytes is the counterpart of dirty_background_ratio. Only<br>one of them may be specified at a time. When one sysctl is written it is<br>immediately taken into account to evaluate the dirty memory limits and the<br>other appears as 0 when read.</p>
<p>==============================================================</p>
<p>dirty_background_ratio</p>
<p>Contains, as a percentage of total system memory, the number of pages at which<br>the background kernel flusher threads will start writing out dirty data.</p>
<p>==============================================================</p>
<p>dirty_bytes</p>
<p>Contains the amount of dirty memory at which a process generating disk writes<br>will itself start writeback.</p>
<p>Note: dirty_bytes is the counterpart of dirty_ratio. Only one of them may be<br>specified at a time. When one sysctl is written it is immediately taken into<br>account to evaluate the dirty memory limits and the other appears as 0 when<br>read.</p>
<p>Note: the minimum value allowed for dirty_bytes is two pages (in bytes); any<br>value lower than this limit will be ignored and the old configuration will be<br>retained.</p>
<p>==============================================================</p>
<p>dirty_expire_centisecs</p>
<p>This tunable is used to define when dirty data is old enough to be eligible<br>for writeout by the kernel flusher threads.  It is expressed in 100’ths<br>of a second.  Data which has been dirty in-memory for longer than this<br>interval will be written out next time a flusher thread wakes up.</p>
<p>==============================================================</p>
<p>dirty_ratio</p>
<p>Contains, as a percentage of total system memory, the number of pages at which<br>a process which is generating disk writes will itself start writing out dirty<br>data.</p>
<p>==============================================================</p>
<p>dirty_writeback_centisecs</p>
<p>The kernel flusher threads will periodically wake up and write `old’ data<br>out to disk.  This tunable expresses the interval between those wakeups, in<br>100’ths of a second.</p>
<p>Setting this to zero disables periodic writeback altogether.</p>
<p>==============================================================</p>
<p>drop_caches</p>
<p>Writing to this will cause the kernel to drop clean caches, as well as<br>reclaimable slab objects like dentries and inodes.  Once dropped, their<br>memory becomes free.</p>
<p>To free pagecache:<br>    echo 1 &gt; /proc/sys/vm/drop_caches<br>To free reclaimable slab objects (includes dentries and inodes):<br>    echo 2 &gt; /proc/sys/vm/drop_caches<br>To free slab objects and pagecache:<br>    echo 3 &gt; /proc/sys/vm/drop_caches</p>
<p>This is a non-destructive operation and will not free any dirty objects.<br>To increase the number of objects freed by this operation, the user may run<br>`sync’ prior to writing to /proc/sys/vm/drop_caches.  This will minimize the<br>number of dirty objects on the system and create more candidates to be<br>dropped.</p>
<p>This file is not a means to control the growth of the various kernel caches<br>(inodes, dentries, pagecache, etc…)  These objects are automatically<br>reclaimed by the kernel when memory is needed elsewhere on the system.</p>
<p>Use of this file can cause performance problems.  Since it discards cached<br>objects, it may cost a significant amount of I/O and CPU to recreate the<br>dropped objects, especially if they were under heavy use.  Because of this,<br>use outside of a testing or debugging environment is not recommended.</p>
<p>You may see informational messages in your kernel log when this file is<br>used:</p>
<pre><code>cat (1234): drop_caches: 3
</code></pre>
<p>These are informational only.  They do not mean that anything is wrong<br>with your system.  To disable them, echo 4 (bit 3) into drop_caches.</p>
<p>==============================================================</p>
<p>extfrag_threshold</p>
<p>This parameter affects whether the kernel will compact memory or direct<br>reclaim to satisfy a high-order allocation. /proc/extfrag_index shows what<br>the fragmentation index for each order is in each zone in the system. Values<br>tending towards 0 imply allocations would fail due to lack of memory,<br>values towards 1000 imply failures are due to fragmentation and -1 implies<br>that the allocation will succeed as long as watermarks are met.</p>
<p>The kernel will not compact memory in a zone if the<br>fragmentation index is &lt;= extfrag_threshold. The default value is 500.</p>
<p>==============================================================</p>
<p>hugepages_treat_as_movable</p>
<p>This parameter controls whether we can allocate hugepages from ZONE_MOVABLE<br>or not. If set to non-zero, hugepages can be allocated from ZONE_MOVABLE.<br>ZONE_MOVABLE is created when kernel boot parameter kernelcore= is specified,<br>so this parameter has no effect if used without kernelcore=.</p>
<p>Hugepage migration is now available in some situations which depend on the<br>architecture and/or the hugepage size. If a hugepage supports migration,<br>allocation from ZONE_MOVABLE is always enabled for the hugepage regardless<br>of the value of this parameter.<br>IOW, this parameter affects only non-migratable hugepages.</p>
<p>Assuming that hugepages are not migratable in your system, one usecase of<br>this parameter is that users can make hugepage pool more extensible by<br>enabling the allocation from ZONE_MOVABLE. This is because on ZONE_MOVABLE<br>page reclaim/migration/compaction work more and you can get contiguous<br>memory more likely. Note that using ZONE_MOVABLE for non-migratable<br>hugepages can do harm to other features like memory hotremove (because<br>memory hotremove expects that memory blocks on ZONE_MOVABLE are always<br>removable,) so it’s a trade-off responsible for the users.</p>
<p>==============================================================</p>
<p>hugetlb_shm_group</p>
<p>hugetlb_shm_group contains group id that is allowed to create SysV<br>shared memory segment using hugetlb page.</p>
<p>==============================================================</p>
<p>laptop_mode</p>
<p>laptop_mode is a knob that controls “laptop mode”. All the things that are<br>controlled by this knob are discussed in Documentation/laptops/laptop-mode.txt.</p>
<p>==============================================================</p>
<p>legacy_va_layout</p>
<p>If non-zero, this sysctl disables the new 32-bit mmap layout - the kernel<br>will use the legacy (2.4) layout for all processes.</p>
<p>==============================================================</p>
<p>lowmem_reserve_ratio</p>
<p>For some specialised workloads on highmem machines it is dangerous for<br>the kernel to allow process memory to be allocated from the “lowmem”<br>zone.  This is because that memory could then be pinned via the mlock()<br>system call, or by unavailability of swapspace.</p>
<p>And on large highmem machines this lack of reclaimable lowmem memory<br>can be fatal.</p>
<p>So the Linux page allocator has a mechanism which prevents allocations<br>which <em>could</em> use highmem from using too much lowmem.  This means that<br>a certain amount of lowmem is defended from the possibility of being<br>captured into pinned user memory.</p>
<p>(The same argument applies to the old 16 megabyte ISA DMA region.  This<br>mechanism will also defend that region from allocations which could use<br>highmem or lowmem).</p>
<p>The `lowmem_reserve_ratio’ tunable determines how aggressive the kernel is<br>in defending these lower zones.</p>
<p>If you have a machine which uses highmem or ISA DMA and your<br>applications are using mlock(), or if you are running with no swap then<br>you probably should change the lowmem_reserve_ratio setting.</p>
<h2 id="The-lowmem-reserve-ratio-is-an-array-You-can-see-them-by-reading-this-file"><a href="#The-lowmem-reserve-ratio-is-an-array-You-can-see-them-by-reading-this-file" class="headerlink" title="The lowmem_reserve_ratio is an array. You can see them by reading this file."></a>The lowmem_reserve_ratio is an array. You can see them by reading this file.</h2><p>% cat /proc/sys/vm/lowmem_reserve_ratio<br>256     256     32<br>-<br>Note: # of this elements is one fewer than number of zones. Because the highest<br>      zone’s value is not necessary for following calculation.</p>
<p>But, these values are not used directly. The kernel calculates # of protection<br>pages for each zones from them. These are shown as array of protection pages<br>in /proc/zoneinfo like followings. (This is an example of x86-64 box).<br>Each zone has an array of protection pages like this.</p>
<p>-<br>Node 0, zone      DMA<br>  pages free     1355<br>        min      3<br>        low      3<br>        high     4<br>    :<br>    :<br>    numa_other   0<br>        protection: (0, 2004, 2004, 2004)<br>    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  pagesets<br>    cpu: 0 pcp: 0<br>        :<br>-<br>These protections are added to score to judge whether this zone should be used<br>for page allocation or should be reclaimed.</p>
<p>In this example, if normal pages (index=2) are required to this DMA zone and<br>watermark[WMARK_HIGH] is used for watermark, the kernel judges this zone should<br>not be used because pages_free(1355) is smaller than watermark + protection[2]<br>(4 + 2004 = 2008). If this protection value is 0, this zone would be used for<br>normal page requirement. If requirement is DMA zone(index=0), protection[0]<br>(=0) is used.</p>
<p>zone[i]’s protection[j] is calculated by following expression.</p>
<p>(i &lt; j):<br>  zone[i]-&gt;protection[j]<br>  = (total sums of present_pages from zone[i+1] to zone[j] on the node)<br>    / lowmem_reserve_ratio[i];<br>(i = j):<br>   (should not be protected. = 0;<br>(i &gt; j):<br>   (not necessary, but looks 0)</p>
<p>The default values of lowmem_reserve_ratio[i] are<br>    256 (if zone[i] means DMA or DMA32 zone)<br>    32  (others).<br>As above expression, they are reciprocal number of ratio.<br>256 means 1/256. # of protection pages becomes about “0.39%” of total present<br>pages of higher zones on the node.</p>
<p>If you would like to protect more pages, smaller values are effective.<br>The minimum value is 1 (1/1 -&gt; 100%).</p>
<p>==============================================================</p>
<p>max_map_count:</p>
<p>This file contains the maximum number of memory map areas a process<br>may have. Memory map areas are used as a side-effect of calling<br>malloc, directly by mmap, mprotect, and madvise, and also when loading<br>shared libraries.</p>
<p>While most applications need less than a thousand maps, certain<br>programs, particularly malloc debuggers, may consume lots of them,<br>e.g., up to one or two maps per allocation.</p>
<p>The default value is 65536.</p>
<p>=============================================================</p>
<p>memory_failure_early_kill:</p>
<p>Control how to kill processes when uncorrected memory error (typically<br>a 2bit error in a memory module) is detected in the background by hardware<br>that cannot be handled by the kernel. In some cases (like the page<br>still having a valid copy on disk) the kernel will handle the failure<br>transparently without affecting any applications. But if there is<br>no other uptodate copy of the data it will kill to prevent any data<br>corruptions from propagating.</p>
<p>1: Kill all processes that have the corrupted and not reloadable page mapped<br>as soon as the corruption is detected.  Note this is not supported<br>for a few types of pages, like kernel internally allocated data or<br>the swap cache, but works for the majority of user pages.</p>
<p>0: Only unmap the corrupted page from all processes and only kill a process<br>who tries to access it.</p>
<p>The kill is done using a catchable SIGBUS with BUS_MCEERR_AO, so processes can<br>handle this if they want to.</p>
<p>This is only active on architectures/platforms with advanced machine<br>check handling and depends on the hardware capabilities.</p>
<p>Applications can override this setting individually with the PR_MCE_KILL prctl</p>
<p>==============================================================</p>
<p>memory_failure_recovery</p>
<p>Enable memory failure recovery (when supported by the platform)</p>
<p>1: Attempt recovery.</p>
<p>0: Always panic on a memory failure.</p>
<p>==============================================================</p>
<p>min_free_kbytes:</p>
<p>This is used to force the Linux VM to keep a minimum number<br>of kilobytes free.  The VM uses this number to compute a<br>watermark[WMARK_MIN] value for each lowmem zone in the system.<br>Each lowmem zone gets a number of reserved free pages based<br>proportionally on its size.</p>
<p>Some minimal amount of memory is needed to satisfy PF_MEMALLOC<br>allocations; if you set this to lower than 1024KB, your system will<br>become subtly broken, and prone to deadlock under high loads.</p>
<p>Setting this too high will OOM your machine instantly.</p>
<p>=============================================================</p>
<p>min_slab_ratio:</p>
<p>This is available only on NUMA kernels.</p>
<p>A percentage of the total pages in each zone.  On Zone reclaim<br>(fallback from the local zone occurs) slabs will be reclaimed if more<br>than this percentage of pages in a zone are reclaimable slab pages.<br>This insures that the slab growth stays under control even in NUMA<br>systems that rarely perform global reclaim.</p>
<p>The default is 5 percent.</p>
<p>Note that slab reclaim is triggered in a per zone / node fashion.<br>The process of reclaiming slab memory is currently not node specific<br>and may not be fast.</p>
<p>=============================================================</p>
<p>min_unmapped_ratio:</p>
<p>This is available only on NUMA kernels.</p>
<p>This is a percentage of the total pages in each zone. Zone reclaim will<br>only occur if more than this percentage of pages are in a state that<br>zone_reclaim_mode allows to be reclaimed.</p>
<p>If zone_reclaim_mode has the value 4 OR’d, then the percentage is compared<br>against all file-backed unmapped pages including swapcache pages and tmpfs<br>files. Otherwise, only unmapped pages backed by normal files but not tmpfs<br>files and similar are considered.</p>
<p>The default is 1 percent.</p>
<p>==============================================================</p>
<p>mmap_min_addr</p>
<p>This file indicates the amount of address space  which a user process will<br>be restricted from mmapping.  Since kernel null dereference bugs could<br>accidentally operate based on the information in the first couple of pages<br>of memory userspace processes should not be allowed to write to them.  By<br>default this value is set to 0 and no protections will be enforced by the<br>security module.  Setting this value to something like 64k will allow the<br>vast majority of applications to work correctly and provide defense in depth<br>against future potential kernel bugs.</p>
<p>==============================================================</p>
<p>mmap_rnd_bits:</p>
<p>This value can be used to select the number of bits to use to<br>determine the random offset to the base address of vma regions<br>resulting from mmap allocations on architectures which support<br>tuning address space randomization.  This value will be bounded<br>by the architecture’s minimum and maximum supported values.</p>
<p>This value can be changed after boot using the<br>/proc/sys/vm/mmap_rnd_bits tunable</p>
<p>==============================================================</p>
<p>mmap_rnd_compat_bits:</p>
<p>This value can be used to select the number of bits to use to<br>determine the random offset to the base address of vma regions<br>resulting from mmap allocations for applications run in<br>compatibility mode on architectures which support tuning address<br>space randomization.  This value will be bounded by the<br>architecture’s minimum and maximum supported values.</p>
<p>This value can be changed after boot using the<br>/proc/sys/vm/mmap_rnd_compat_bits tunable</p>
<p>==============================================================</p>
<p>nr_hugepages</p>
<p>Change the minimum size of the hugepage pool.</p>
<p>See Documentation/vm/hugetlbpage.txt</p>
<p>==============================================================</p>
<p>nr_overcommit_hugepages</p>
<p>Change the maximum size of the hugepage pool. The maximum is<br>nr_hugepages + nr_overcommit_hugepages.</p>
<p>See Documentation/vm/hugetlbpage.txt</p>
<p>==============================================================</p>
<p>nr_trim_pages</p>
<p>This is available only on NOMMU kernels.</p>
<p>This value adjusts the excess page trimming behaviour of power-of-2 aligned<br>NOMMU mmap allocations.</p>
<p>A value of 0 disables trimming of allocations entirely, while a value of 1<br>trims excess pages aggressively. Any value &gt;= 1 acts as the watermark where<br>trimming of allocations is initiated.</p>
<p>The default value is 1.</p>
<p>See Documentation/nommu-mmap.txt for more information.</p>
<p>==============================================================</p>
<p>numa_zonelist_order</p>
<p>This sysctl is only for NUMA.<br>‘where the memory is allocated from’ is controlled by zonelists.<br>(This documentation ignores ZONE_HIGHMEM/ZONE_DMA32 for simple explanation.<br> you may be able to read ZONE_DMA as ZONE_DMA32…)</p>
<p>In non-NUMA case, a zonelist for GFP_KERNEL is ordered as following.<br>ZONE_NORMAL -&gt; ZONE_DMA<br>This means that a memory allocation request for GFP_KERNEL will<br>get memory from ZONE_DMA only when ZONE_NORMAL is not available.</p>
<p>In NUMA case, you can think of following 2 types of order.<br>Assume 2 node NUMA and below is zonelist of Node(0)’s GFP_KERNEL</p>
<p>(A) Node(0) ZONE_NORMAL -&gt; Node(0) ZONE_DMA -&gt; Node(1) ZONE_NORMAL<br>(B) Node(0) ZONE_NORMAL -&gt; Node(1) ZONE_NORMAL -&gt; Node(0) ZONE_DMA.</p>
<p>Type(A) offers the best locality for processes on Node(0), but ZONE_DMA<br>will be used before ZONE_NORMAL exhaustion. This increases possibility of<br>out-of-memory(OOM) of ZONE_DMA because ZONE_DMA is tend to be small.</p>
<p>Type(B) cannot offer the best locality but is more robust against OOM of<br>the DMA zone.</p>
<p>Type(A) is called as “Node” order. Type (B) is “Zone” order.</p>
<p>“Node order” orders the zonelists by node, then by zone within each node.<br>Specify “[Nn]ode” for node order</p>
<p>“Zone Order” orders the zonelists by zone type, then by node within each<br>zone.  Specify “[Zz]one” for zone order.</p>
<p>Specify “[Dd]efault” to request automatic configuration.  Autoconfiguration<br>will select “node” order in following case.<br>(1) if the DMA zone does not exist or<br>(2) if the DMA zone comprises greater than 50% of the available memory or<br>(3) if any node’s DMA zone comprises greater than 60% of its local memory and<br>    the amount of local memory is big enough.</p>
<p>Otherwise, “zone” order will be selected. Default order is recommended unless<br>this is causing problems for your system/application.</p>
<p>==============================================================</p>
<p>oom_dump_tasks</p>
<p>Enables a system-wide task dump (excluding kernel threads) to be<br>produced when the kernel performs an OOM-killing and includes such<br>information as pid, uid, tgid, vm size, rss, nr_ptes, swapents,<br>oom_score_adj score, and name.  This is helpful to determine why the<br>OOM killer was invoked, to identify the rogue task that caused it,<br>and to determine why the OOM killer chose the task it did to kill.</p>
<p>If this is set to zero, this information is suppressed.  On very<br>large systems with thousands of tasks it may not be feasible to dump<br>the memory state information for each one.  Such systems should not<br>be forced to incur a performance penalty in OOM conditions when the<br>information may not be desired.</p>
<p>If this is set to non-zero, this information is shown whenever the<br>OOM killer actually kills a memory-hogging task.</p>
<p>The default value is 1 (enabled).</p>
<p>==============================================================</p>
<p>oom_kill_allocating_task</p>
<p>This enables or disables killing the OOM-triggering task in<br>out-of-memory situations.</p>
<p>If this is set to zero, the OOM killer will scan through the entire<br>tasklist and select a task based on heuristics to kill.  This normally<br>selects a rogue memory-hogging task that frees up a large amount of<br>memory when killed.</p>
<p>If this is set to non-zero, the OOM killer simply kills the task that<br>triggered the out-of-memory condition.  This avoids the expensive<br>tasklist scan.</p>
<p>If panic_on_oom is selected, it takes precedence over whatever value<br>is used in oom_kill_allocating_task.</p>
<p>The default value is 0.</p>
<p>==============================================================</p>
<p>overcommit_kbytes:</p>
<p>When overcommit_memory is set to 2, the committed address space is not<br>permitted to exceed swap plus this amount of physical RAM. See below.</p>
<p>Note: overcommit_kbytes is the counterpart of overcommit_ratio. Only one<br>of them may be specified at a time. Setting one disables the other (which<br>then appears as 0 when read).</p>
<p>==============================================================</p>
<p>overcommit_memory:</p>
<p>This value contains a flag that enables memory overcommitment.</p>
<p>When this flag is 0, the kernel attempts to estimate the amount<br>of free memory left when userspace requests more memory.</p>
<p>When this flag is 1, the kernel pretends there is always enough<br>memory until it actually runs out.</p>
<p>When this flag is 2, the kernel uses a “never overcommit”<br>policy that attempts to prevent any overcommit of memory.<br>Note that user_reserve_kbytes affects this policy.</p>
<p>This feature can be very useful because there are a lot of<br>programs that malloc() huge amounts of memory “just-in-case”<br>and don’t use much of it.</p>
<p>The default value is 0.</p>
<p>See Documentation/vm/overcommit-accounting and<br>security/commoncap.c::cap_vm_enough_memory() for more information.</p>
<p>==============================================================</p>
<p>overcommit_ratio:</p>
<p>When overcommit_memory is set to 2, the committed address<br>space is not permitted to exceed swap plus this percentage<br>of physical RAM.  See above.</p>
<p>==============================================================</p>
<p>page-cluster</p>
<p>page-cluster controls the number of pages up to which consecutive pages<br>are read in from swap in a single attempt. This is the swap counterpart<br>to page cache readahead.<br>The mentioned consecutivity is not in terms of virtual/physical addresses,<br>but consecutive on swap space - that means they were swapped out together.</p>
<p>It is a logarithmic value - setting it to zero means “1 page”, setting<br>it to 1 means “2 pages”, setting it to 2 means “4 pages”, etc.<br>Zero disables swap readahead completely.</p>
<p>The default value is three (eight pages at a time).  There may be some<br>small benefits in tuning this to a different value if your workload is<br>swap-intensive.</p>
<p>Lower values mean lower latencies for initial faults, but at the same time<br>extra faults and I/O delays for following faults if they would have been part of<br>that consecutive pages readahead would have brought in.</p>
<p>=============================================================</p>
<p>panic_on_oom</p>
<p>This enables or disables panic on out-of-memory feature.</p>
<p>If this is set to 0, the kernel will kill some rogue process,<br>called oom_killer.  Usually, oom_killer can kill rogue processes and<br>system will survive.</p>
<p>If this is set to 1, the kernel panics when out-of-memory happens.<br>However, if a process limits using nodes by mempolicy/cpusets,<br>and those nodes become memory exhaustion status, one process<br>may be killed by oom-killer. No panic occurs in this case.<br>Because other nodes’ memory may be free. This means system total status<br>may be not fatal yet.</p>
<p>If this is set to 2, the kernel panics compulsorily even on the<br>above-mentioned. Even oom happens under memory cgroup, the whole<br>system panics.</p>
<p>The default value is 0.<br>1 and 2 are for failover of clustering. Please select either<br>according to your policy of failover.<br>panic_on_oom=2+kdump gives you very strong tool to investigate<br>why oom happens. You can get snapshot.</p>
<p>=============================================================</p>
<p>percpu_pagelist_fraction</p>
<p>This is the fraction of pages at most (high mark pcp-&gt;high) in each zone that<br>are allocated for each per cpu page list.  The min value for this is 8.  It<br>means that we don’t allow more than 1/8th of pages in each zone to be<br>allocated in any single per_cpu_pagelist.  This entry only changes the value<br>of hot per cpu pagelists.  User can specify a number like 100 to allocate<br>1/100th of each zone to each per cpu page list.</p>
<p>The batch value of each per cpu pagelist is also updated as a result.  It is<br>set to pcp-&gt;high/4.  The upper limit of batch is (PAGE_SHIFT * 8)</p>
<p>The initial value is zero.  Kernel does not use this value at boot time to set<br>the high water marks for each per cpu page list.  If the user writes ‘0’ to this<br>sysctl, it will revert to this default behavior.</p>
<p>==============================================================</p>
<p>stat_interval</p>
<p>The time interval between which vm statistics are updated.  The default<br>is 1 second.</p>
<p>==============================================================</p>
<p>swappiness</p>
<p>This control is used to define how aggressive the kernel will swap<br>memory pages.  Higher values will increase agressiveness, lower values<br>decrease the amount of swap.  A value of 0 instructs the kernel not to<br>initiate swap until the amount of free and file-backed pages is less<br>than the high water mark in a zone.</p>
<p>The default value is 60.</p>
<p>==============================================================</p>
<ul>
<li>user_reserve_kbytes</li>
</ul>
<p>When overcommit_memory is set to 2, “never overommit” mode, reserve<br>min(3% of current process size, user_reserve_kbytes) of free memory.<br>This is intended to prevent a user from starting a single memory hogging<br>process, such that they cannot recover (kill the hog).</p>
<p>user_reserve_kbytes defaults to min(3% of the current process size, 128MB).</p>
<p>If this is reduced to zero, then the user will be allowed to allocate<br>all free memory with a single process, minus admin_reserve_kbytes.<br>Any subsequent attempts to execute a command will result in<br>“fork: Cannot allocate memory”.</p>
<p>Changing this takes effect whenever an application requests memory.</p>
<p>==============================================================</p>
<h2 id="vfs-cache-pressure"><a href="#vfs-cache-pressure" class="headerlink" title="vfs_cache_pressure"></a>vfs_cache_pressure</h2><p>Controls the tendency of the kernel to reclaim the memory which is used for<br>caching of directory and inode objects.</p>
<p>At the default value of vfs_cache_pressure=100 the kernel will attempt to<br>reclaim dentries and inodes at a “fair” rate with respect to pagecache and<br>swapcache reclaim.  Decreasing vfs_cache_pressure causes the kernel to prefer<br>to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will<br>never reclaim dentries and inodes due to memory pressure and this can easily<br>lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100<br>causes the kernel to prefer to reclaim dentries and inodes.</p>
<p>==============================================================</p>
<p>zone_reclaim_mode:</p>
<p>Zone_reclaim_mode allows someone to set more or less aggressive approaches to<br>reclaim memory when a zone runs out of memory. If it is set to zero then no<br>zone reclaim occurs. Allocations will be satisfied from other zones / nodes<br>in the system.</p>
<p>This is value ORed together of</p>
<p>1    = Zone reclaim on<br>2    = Zone reclaim writes dirty pages out<br>4    = Zone reclaim swaps pages</p>
<p>zone_reclaim_mode is disabled by default.  For file servers or workloads<br>that benefit from having their data cached, zone_reclaim_mode should be<br>left disabled as the caching effect is likely to be more important than<br>data locality.</p>
<p>zone_reclaim may be enabled if it’s known that the workload is partitioned<br>such that each partition fits within a NUMA node and that accessing remote<br>memory would cause a measurable performance reduction.  The page allocator<br>will then reclaim easily reusable pages (those page cache pages that are<br>currently not used) before allocating off node pages.</p>
<p>Allowing zone reclaim to write out pages stops processes that are<br>writing large amounts of data from dirtying pages on other nodes. Zone<br>reclaim will write out dirty pages if a zone fills up and so effectively<br>throttle the process. This may decrease the performance of a single process<br>since it cannot use all of system memory to buffer the outgoing writes<br>anymore but it preserve the memory on other nodes so that the performance<br>of other processes running on other nodes will not be affected.</p>
<p>Allowing regular swap effectively restricts allocations to the local<br>node unless explicitly overridden by memory policies or cpuset<br>configurations.</p>
<p>============ End of Document =================================</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_vm/" title="Kernel-3.10.0-957.el7_v" target="_blank" rel="external">http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_vm/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">oosTech by Sam Lee</span><small class="ml-1x"></small></a></h3>
        <div></div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    

  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2021/03/13/Kernel-3.10.0-957.el7_watchdog-parameters/" title="Kernel-3.10.0-957.el7_watchdog-parameters"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2021/03/13/Kernel-3.10.0-957.el7_v4l2-controls/" title="Kernel-3.10.0-957.el7_v4l2-controls"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">
        <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,wechat"></div>
    
  </div>
  </div>
    <div class="container" align="left">
    <div class="post-gallery" itemscope itemtype="http://schema.org/ImageGallery">
      <img src="/images/wechatp.png" itemprop="contentUrl">
      <span>欢迎关注公众号</span>
    </div>
    </div>
</nav>

  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>您能有收获就是我们最大的动力</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">不管多少都是对分享的肯定</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码捐赠</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">不管多少都是对分享的肯定</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码捐赠</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>




</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://google.com/" target="_blank" title="Google" data-toggle=tooltip data-placement=top><i class="icon icon-google"></i></a></li>
        
        <li><a href="https://twitter.com/" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
	<a target="_blank" rel="noopener" href="https://beian.miit.gov.cn" text-align: center >粤ICP备2021024811号</a>
        </div>
<!--    
    <div>
     <img src="/images/logo.png" width="140" height="140">
    </div>
-->
    </div>


        <!-- 不蒜子统计    //busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js -->
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <!--<span id="busuanzi_container_site_pv" style="display:inline;">-->本站总访问量<span id="busuanzi_value_site_pv" style="display:inline;></span>次</span>
        <span class="post-meta-divider">|</span>
  
</footer>

  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>







   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>











</body>
</html>