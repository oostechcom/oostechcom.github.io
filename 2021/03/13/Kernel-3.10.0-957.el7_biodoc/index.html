<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>Kernel-3.10.0-957.el7_biodoc | oosTech.com</title>
  <meta name="description" content="Notes on the Generic Block Layer Rewrite in Linux 2.5 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;  Notes Written on Jan 15, 2002:    Jens Axboe &amp;#106;&amp;#101;&amp;#x6e;&amp;#115;&amp;#x2e;&amp;#x61;&amp;#120;&amp;#98">
<meta property="og:type" content="article">
<meta property="og:title" content="Kernel-3.10.0-957.el7_biodoc">
<meta property="og:url" content="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_biodoc/index.html">
<meta property="og:site_name" content="oosTech">
<meta property="og:description" content="Notes on the Generic Block Layer Rewrite in Linux 2.5 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;  Notes Written on Jan 15, 2002:    Jens Axboe &amp;#106;&amp;#101;&amp;#x6e;&amp;#115;&amp;#x2e;&amp;#x61;&amp;#120;&amp;#98">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-03-12T16:00:00.000Z">
<meta property="article:modified_time" content="2021-03-12T16:00:00.000Z">
<meta property="article:author" content="Sam Lee">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_biodoc/index.html">
  
    <link rel="alternate" href="/atom.xml" title="oosTech" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img src="/images/avatar.png" width="400" height="400">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">oosTech by Sam Lee</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="站内搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">站点首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档文档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">文档分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">常用链接</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">共享白板</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://google.com/" target="_blank" title="Google" data-toggle=tooltip data-placement=top><i class="icon icon-google"></i></a></li>
        
        <li><a href="https://twitter.com/" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告牌</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p> 欢迎来到oosTech ，我是一个Linux 拥趸：D。 目前正在用的Linux 版本是 Red Hat Enterprise Linux release 8.3 </p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">文档分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-2-6-32-573-12-1-el6-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_2.6.32-573.12.1.el6_内核文档</a><span class="category-list-count">830</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a><span class="category-list-count">1658</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kernel-4-18-0-80-el8-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_4.18.0-80.el8_内核文档</a><span class="category-list-count">3937</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_3270/" class="title">Kernel-3.10.0-957.el7_3270</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-2-6-32-573-12-1-el6-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_2.6.32-573.12.1.el6_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-2.6.32-573.12.1.el6_devices/" class="title">Kernel-2.6.32-573.12.1.el6_devices</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_3c509/" class="title">Kernel-3.10.0-957.el7_3c509</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_4CCs/" class="title">Kernel-3.10.0-957.el7_4CCs</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
              </p>
              <p class="item-title">
                <a href="/2021/03/13/Kernel-3.10.0-957.el7_53c700/" class="title">Kernel-3.10.0-957.el7_53c700</a>
              </p>
              <p class="item-date">
                <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Credits"><span class="toc-number">1.</span> <span class="toc-text">Credits:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Description-of-Contents"><span class="toc-number">2.</span> <span class="toc-text">Description of Contents:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bio-Notes"><span class="toc-number">3.</span> <span class="toc-text">Bio Notes</span></a></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-Kernel-3.10.0-957.el7_biodoc" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Kernel-3.10.0-957.el7_biodoc
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2021/03/13/Kernel-3.10.0-957.el7_biodoc/" class="article-date">
	 published: <time datetime="2021-03-12T16:00:00.000Z" itemprop="datePublished">2021-03-13</time>
	</a>
</span>

        
	<a href="/2021/03/13/Kernel-3.10.0-957.el7_biodoc/" class="article-date">
	   updated: <time datetime="2021-03-12T16:00:00.000Z" itemprop="dateUpdated">2021-03-13</time>
	</a>


        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/Kernel-3-10-0-957-el7-%E5%86%85%E6%A0%B8%E6%96%87%E6%A1%A3/">Kernel_3.10.0-957.el7_内核文档</a>
  </span>

        

        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill"></i>
	    <!--<span id="busuanzi_container_page_pv" style="display:inline;">-->
			<span id="busuanzi_value_page_pv" style="display:inline;"></span>
		<!--</span>-->
	</span>



        <!--<span class="post-comment"><i class="icon icon-comment"></i> <a href="/2021/03/13/Kernel-3.10.0-957.el7_biodoc/#comments" class="article-comment-link">评论</a></span> -->
        
	
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <pre><code>Notes on the Generic Block Layer Rewrite in Linux 2.5
=====================================================
</code></pre>
<p>Notes Written on Jan 15, 2002:<br>    Jens Axboe <a href="mailto:&#106;&#101;&#x6e;&#115;&#x2e;&#x61;&#120;&#98;&#x6f;&#x65;&#x40;&#x6f;&#x72;&#97;&#x63;&#108;&#x65;&#x2e;&#x63;&#111;&#109;">&#106;&#101;&#x6e;&#115;&#x2e;&#x61;&#120;&#98;&#x6f;&#x65;&#x40;&#x6f;&#x72;&#97;&#x63;&#108;&#x65;&#x2e;&#x63;&#111;&#109;</a><br>    Suparna Bhattacharya <a href="mailto:&#x73;&#117;&#x70;&#97;&#114;&#110;&#x61;&#64;&#105;&#110;&#46;&#x69;&#98;&#109;&#46;&#99;&#111;&#x6d;">&#x73;&#117;&#x70;&#97;&#114;&#110;&#x61;&#64;&#105;&#110;&#46;&#x69;&#98;&#109;&#46;&#99;&#111;&#x6d;</a></p>
<p>Last Updated May 2, 2002<br>September 2003: Updated I/O Scheduler portions<br>    Nick Piggin <a href="mailto:&#110;&#x70;&#x69;&#103;&#x67;&#x69;&#x6e;&#x40;&#x6b;&#x65;&#x72;&#110;&#101;&#x6c;&#x2e;&#100;&#107;">&#110;&#x70;&#x69;&#103;&#x67;&#x69;&#x6e;&#x40;&#x6b;&#x65;&#x72;&#110;&#101;&#x6c;&#x2e;&#100;&#107;</a></p>
<p>Introduction:</p>
<p>These are some notes describing some aspects of the 2.5 block layer in the<br>context of the bio rewrite. The idea is to bring out some of the key<br>changes and a glimpse of the rationale behind those changes.</p>
<p>Please mail corrections &amp; suggestions to <a href="mailto:&#115;&#117;&#x70;&#x61;&#114;&#110;&#x61;&#x40;&#105;&#x6e;&#x2e;&#x69;&#x62;&#x6d;&#46;&#99;&#111;&#109;">&#115;&#117;&#x70;&#x61;&#114;&#110;&#x61;&#x40;&#105;&#x6e;&#x2e;&#x69;&#x62;&#x6d;&#46;&#99;&#111;&#109;</a>.</p>
<h2 id="Credits"><a href="#Credits" class="headerlink" title="Credits:"></a>Credits:</h2><p>2.5 bio rewrite:<br>    Jens Axboe <a href="mailto:&#x6a;&#x65;&#x6e;&#x73;&#46;&#97;&#120;&#98;&#x6f;&#x65;&#64;&#111;&#114;&#x61;&#x63;&#108;&#x65;&#46;&#x63;&#111;&#109;">&#x6a;&#x65;&#x6e;&#x73;&#46;&#97;&#120;&#98;&#x6f;&#x65;&#64;&#111;&#114;&#x61;&#x63;&#108;&#x65;&#46;&#x63;&#111;&#109;</a></p>
<p>Many aspects of the generic block layer redesign were driven by and evolved<br>over discussions, prior patches and the collective experience of several<br>people. See sections 8 and 9 for a list of some related references.</p>
<p>The following people helped with review comments and inputs for this<br>document:<br>    Christoph Hellwig <a href="mailto:&#104;&#x63;&#104;&#x40;&#x69;&#x6e;&#x66;&#114;&#x61;&#x64;&#101;&#97;&#100;&#x2e;&#111;&#114;&#x67;">&#104;&#x63;&#104;&#x40;&#x69;&#x6e;&#x66;&#114;&#x61;&#x64;&#101;&#97;&#100;&#x2e;&#111;&#114;&#x67;</a><br>    Arjan van de Ven <a href="mailto:&#x61;&#114;&#106;&#97;&#x6e;&#118;&#x40;&#114;&#101;&#100;&#104;&#97;&#x74;&#46;&#x63;&#x6f;&#x6d;">&#x61;&#114;&#106;&#97;&#x6e;&#118;&#x40;&#114;&#101;&#100;&#104;&#97;&#x74;&#46;&#x63;&#x6f;&#x6d;</a><br>    Randy Dunlap <a href="mailto:&#x72;&#100;&#117;&#x6e;&#x6c;&#x61;&#112;&#64;&#120;&#x65;&#x6e;&#111;&#116;&#x69;&#x6d;&#x65;&#x2e;&#110;&#x65;&#x74;">&#x72;&#100;&#117;&#x6e;&#x6c;&#x61;&#112;&#64;&#120;&#x65;&#x6e;&#111;&#116;&#x69;&#x6d;&#x65;&#x2e;&#110;&#x65;&#x74;</a><br>    Andre Hedrick <a href="mailto:&#x61;&#x6e;&#x64;&#114;&#101;&#x40;&#x6c;&#105;&#110;&#117;&#120;&#45;&#105;&#100;&#x65;&#x2e;&#111;&#114;&#103;">&#x61;&#x6e;&#x64;&#114;&#101;&#x40;&#x6c;&#105;&#110;&#117;&#120;&#45;&#105;&#100;&#x65;&#x2e;&#111;&#114;&#103;</a></p>
<p>The following people helped with fixes/contributions to the bio patches<br>while it was still work-in-progress:<br>    David S. Miller <a href="mailto:&#100;&#x61;&#x76;&#101;&#x6d;&#64;&#x72;&#101;&#100;&#104;&#97;&#116;&#x2e;&#x63;&#111;&#x6d;">&#100;&#x61;&#x76;&#101;&#x6d;&#64;&#x72;&#101;&#100;&#104;&#97;&#116;&#x2e;&#x63;&#111;&#x6d;</a></p>
<h2 id="Description-of-Contents"><a href="#Description-of-Contents" class="headerlink" title="Description of Contents:"></a>Description of Contents:</h2><ol>
<li>Scope for tuning of logic to various needs</li>
<li>1 Tuning based on device or low level driver capabilities<ul>
<li>Per-queue parameters</li>
<li>Highmem I/O support</li>
<li>I/O scheduler modularization</li>
</ul>
</li>
<li>2 Tuning based on high level requirements/capabilities<br>1.2.1 I/O Barriers<br>1.2.2 Request Priority/Latency</li>
<li>3 Direct access/bypass to lower layers for diagnostics and special<br>  device operations<br>1.3.1 Pre-built commands</li>
<li>New flexible and generic but minimalist i/o structure or descriptor<br>(instead of using buffer heads at the i/o layer)</li>
<li>1 Requirements/Goals addressed</li>
<li>2 The bio struct in detail (multi-page io unit)</li>
<li>3 Changes in the request structure</li>
<li>Using bios</li>
<li>1 Setup/teardown (allocation, splitting)</li>
<li>2 Generic bio helper routines<br>3.2.1 Traversing segments and completion units in a request<br>3.2.2 Setting up DMA scatterlists<br>3.2.3 I/O completion<br>3.2.4 Implications for drivers that do not interpret bios (don’t handle<br>   multiple segments)<br>3.2.5 Request command tagging</li>
<li>3 I/O submission</li>
<li>The I/O scheduler</li>
<li>Scalability related changes</li>
<li>1 Granular locking: Removal of io_request_lock</li>
<li>2 Prepare for transition to 64 bit sector_t</li>
<li>Other Changes/Implications</li>
<li>1 Partition re-mapping handled by the generic block layer</li>
<li>A few tips on migration of older drivers</li>
<li>A list of prior/related/impacted patches/ideas</li>
<li>Other References/Discussion Threads</li>
</ol>
<hr>
<h2 id="Bio-Notes"><a href="#Bio-Notes" class="headerlink" title="Bio Notes"></a>Bio Notes</h2><p>Let us discuss the changes in the context of how some overall goals for the<br>block layer are addressed.</p>
<ol>
<li>Scope for tuning the generic logic to satisfy various requirements</li>
</ol>
<p>The block layer design supports adaptable abstractions to handle common<br>processing with the ability to tune the logic to an appropriate extent<br>depending on the nature of the device and the requirements of the caller.<br>One of the objectives of the rewrite was to increase the degree of tunability<br>and to enable higher level code to utilize underlying device/driver<br>capabilities to the maximum extent for better i/o performance. This is<br>important especially in the light of ever improving hardware capabilities<br>and application/middleware software designed to take advantage of these<br>capabilities.</p>
<p>1.1 Tuning based on low level device / driver capabilities</p>
<p>Sophisticated devices with large built-in caches, intelligent i/o scheduling<br>optimizations, high memory DMA support, etc may find some of the<br>generic processing an overhead, while for less capable devices the<br>generic functionality is essential for performance or correctness reasons.<br>Knowledge of some of the capabilities or parameters of the device should be<br>used at the generic block layer to take the right decisions on<br>behalf of the driver.</p>
<p>How is this achieved ?</p>
<p>Tuning at a per-queue level:</p>
<p>i. Per-queue limits/values exported to the generic layer by the driver</p>
<p>Various parameters that the generic i/o scheduler logic uses are set at<br>a per-queue level (e.g maximum request size, maximum number of segments in<br>a scatter-gather list, hardsect size)</p>
<p>Some parameters that were earlier available as global arrays indexed by<br>major/minor are now directly associated with the queue. Some of these may<br>move into the block device structure in the future. Some characteristics<br>have been incorporated into a queue flags field rather than separate fields<br>in themselves.  There are blk_queue_xxx functions to set the parameters,<br>rather than update the fields directly</p>
<p>Some new queue property settings:</p>
<pre><code>blk_queue_bounce_limit(q, u64 dma_address)
    Enable I/O to highmem pages, dma_address being the
    limit. No highmem default.

blk_queue_max_sectors(q, max_sectors)
    Sets two variables that limit the size of the request.

    - The request queue&#39;s max_sectors, which is a soft size in
    units of 512 byte sectors, and could be dynamically varied
    by the core kernel.

    - The request queue&#39;s max_hw_sectors, which is a hard limit
    and reflects the maximum size request a driver can handle
    in units of 512 byte sectors.

    The default for both max_sectors and max_hw_sectors is
    255. The upper limit of max_sectors is 1024.

blk_queue_max_phys_segments(q, max_segments)
    Maximum physical segments you can handle in a request. 128
    default (driver limit). (See 3.2.2)

blk_queue_max_hw_segments(q, max_segments)
    Maximum dma segments the hardware can handle in a request. 128
    default (host adapter limit, after dma remapping).
    (See 3.2.2)

blk_queue_max_segment_size(q, max_seg_size)
    Maximum size of a clustered segment, 64kB default.

blk_queue_hardsect_size(q, hardsect_size)
    Lowest possible sector size that the hardware can operate
    on, 512 bytes default.
</code></pre>
<p>New queue flags:</p>
<pre><code>QUEUE_FLAG_CLUSTER (see 3.2.2)
QUEUE_FLAG_QUEUED (see 3.2.4)
</code></pre>
<p>ii. High-mem i/o capabilities are now considered the default</p>
<p>The generic bounce buffer logic, present in 2.4, where the block layer would<br>by default copyin/out i/o requests on high-memory buffers to low-memory buffers<br>assuming that the driver wouldn’t be able to handle it directly, has been<br>changed in 2.5. The bounce logic is now applied only for memory ranges<br>for which the device cannot handle i/o. A driver can specify this by<br>setting the queue bounce limit for the request queue for the device<br>(blk_queue_bounce_limit()). This avoids the inefficiencies of the copyin/out<br>where a device is capable of handling high memory i/o.</p>
<p>In order to enable high-memory i/o where the device is capable of supporting<br>it, the pci dma mapping routines and associated data structures have now been<br>modified to accomplish a direct page -&gt; bus translation, without requiring<br>a virtual address mapping (unlike the earlier scheme of virtual address<br>-&gt; bus translation). So this works uniformly for high-memory pages (which<br>do not have a corresponding kernel virtual address space mapping) and<br>low-memory pages.</p>
<p>Note: Please refer to Documentation/DMA-API-HOWTO.txt for a discussion<br>on PCI high mem DMA aspects and mapping of scatter gather lists, and support<br>for 64 bit PCI.</p>
<p>Special handling is required only for cases where i/o needs to happen on<br>pages at physical memory addresses beyond what the device can support. In these<br>cases, a bounce bio representing a buffer from the supported memory range<br>is used for performing the i/o with copyin/copyout as needed depending on<br>the type of the operation.  For example, in case of a read operation, the<br>data read has to be copied to the original buffer on i/o completion, so a<br>callback routine is set up to do this, while for write, the data is copied<br>from the original buffer to the bounce buffer prior to issuing the<br>operation. Since an original buffer may be in a high memory area that’s not<br>mapped in kernel virtual addr, a kmap operation may be required for<br>performing the copy, and special care may be needed in the completion path<br>as it may not be in irq context. Special care is also required (by way of<br>GFP flags) when allocating bounce buffers, to avoid certain highmem<br>deadlock possibilities.</p>
<p>It is also possible that a bounce buffer may be allocated from high-memory<br>area that’s not mapped in kernel virtual addr, but within the range that the<br>device can use directly; so the bounce page may need to be kmapped during<br>copy operations. [Note: This does not hold in the current implementation,<br>though]</p>
<p>There are some situations when pages from high memory may need to<br>be kmapped, even if bounce buffers are not necessary. For example a device<br>may need to abort DMA operations and revert to PIO for the transfer, in<br>which case a virtual mapping of the page is required. For SCSI it is also<br>done in some scenarios where the low level driver cannot be trusted to<br>handle a single sg entry correctly. The driver is expected to perform the<br>kmaps as needed on such occasions using the __bio_kmap_atomic and bio_kmap_irq<br>routines as appropriate. A driver could also use the blk_queue_bounce()<br>routine on its own to bounce highmem i/o to low memory for specific requests<br>if so desired.</p>
<p>iii. The i/o scheduler algorithm itself can be replaced/set as appropriate</p>
<p>As in 2.4, it is possible to plugin a brand new i/o scheduler for a particular<br>queue or pick from (copy) existing generic schedulers and replace/override<br>certain portions of it. The 2.5 rewrite provides improved modularization<br>of the i/o scheduler. There are more pluggable callbacks, e.g for init,<br>add request, extract request, which makes it possible to abstract specific<br>i/o scheduling algorithm aspects and details outside of the generic loop.<br>It also makes it possible to completely hide the implementation details of<br>the i/o scheduler from block drivers.</p>
<p>I/O scheduler wrappers are to be used instead of accessing the queue directly.<br>See section 4. The I/O scheduler for details.</p>
<p>1.2 Tuning Based on High level code capabilities</p>
<p>i. Application capabilities for raw i/o</p>
<p>This comes from some of the high-performance database/middleware<br>requirements where an application prefers to make its own i/o scheduling<br>decisions based on an understanding of the access patterns and i/o<br>characteristics</p>
<p>ii. High performance filesystems or other higher level kernel code’s<br>capabilities</p>
<p>Kernel components like filesystems could also take their own i/o scheduling<br>decisions for optimizing performance. Journalling filesystems may need<br>some control over i/o ordering.</p>
<p>What kind of support exists at the generic block layer for this ?</p>
<p>The flags and rw fields in the bio structure can be used for some tuning<br>from above e.g indicating that an i/o is just a readahead request, or for<br>marking  barrier requests (discussed next), or priority settings (currently<br>unused). As far as user applications are concerned they would need an<br>additional mechanism either via open flags or ioctls, or some other upper<br>level mechanism to communicate such settings to block.</p>
<p>1.2.1 I/O Barriers</p>
<p>There is a way to enforce strict ordering for i/os through barriers.<br>All requests before a barrier point must be serviced before the barrier<br>request and any other requests arriving after the barrier will not be<br>serviced until after the barrier has completed. This is useful for higher<br>level control on write ordering, e.g flushing a log of committed updates<br>to disk before the corresponding updates themselves.</p>
<p>A flag in the bio structure, BIO_BARRIER is used to identify a barrier i/o.<br>The generic i/o scheduler would make sure that it places the barrier request and<br>all other requests coming after it after all the previous requests in the<br>queue. Barriers may be implemented in different ways depending on the<br>driver. For more details regarding I/O barriers, please read barrier.txt<br>in this directory.</p>
<p>1.2.2 Request Priority/Latency</p>
<p>Todo/Under discussion:<br>Arjan’s proposed request priority scheme allows higher levels some broad<br>  control (high/med/low) over the priority  of an i/o request vs other pending<br>  requests in the queue. For example it allows reads for bringing in an<br>  executable page on demand to be given a higher priority over pending write<br>  requests which haven’t aged too much on the queue. Potentially this priority<br>  could even be exposed to applications in some manner, providing higher level<br>  tunability. Time based aging avoids starvation of lower priority<br>  requests. Some bits in the bi_rw flags field in the bio structure are<br>  intended to be used for this priority information.</p>
<p>1.3 Direct Access to Low level Device/Driver Capabilities (Bypass mode)<br>    (e.g Diagnostics, Systems Management)</p>
<p>There are situations where high-level code needs to have direct access to<br>the low level device capabilities or requires the ability to issue commands<br>to the device bypassing some of the intermediate i/o layers.<br>These could, for example, be special control commands issued through ioctl<br>interfaces, or could be raw read/write commands that stress the drive’s<br>capabilities for certain kinds of fitness tests. Having direct interfaces at<br>multiple levels without having to pass through upper layers makes<br>it possible to perform bottom up validation of the i/o path, layer by<br>layer, starting from the media.</p>
<p>The normal i/o submission interfaces, e.g submit_bio, could be bypassed<br>for specially crafted requests which such ioctl or diagnostics<br>interfaces would typically use, and the elevator add_request routine<br>can instead be used to directly insert such requests in the queue or preferably<br>the blk_do_rq routine can be used to place the request on the queue and<br>wait for completion. Alternatively, sometimes the caller might just<br>invoke a lower level driver specific interface with the request as a<br>parameter.</p>
<p>If the request is a means for passing on special information associated with<br>the command, then such information is associated with the request-&gt;special<br>field (rather than misuse the request-&gt;buffer field which is meant for the<br>request data buffer’s virtual mapping).</p>
<p>For passing request data, the caller must build up a bio descriptor<br>representing the concerned memory buffer if the underlying driver interprets<br>bio segments or uses the block layer end<em>request</em> functions for i/o<br>completion. Alternatively one could directly use the request-&gt;buffer field to<br>specify the virtual address of the buffer, if the driver expects buffer<br>addresses passed in this way and ignores bio entries for the request type<br>involved. In the latter case, the driver would modify and manage the<br>request-&gt;buffer, request-&gt;sector and request-&gt;nr_sectors or<br>request-&gt;current_nr_sectors fields itself rather than using the block layer<br>end_request or end_that_request_first completion interfaces.<br>(See 2.3 or Documentation/block/request.txt for a brief explanation of<br>the request structure fields)</p>
<p>[TBD: end_that_request_last should be usable even in this case;<br>Perhaps an end_that_direct_request_first routine could be implemented to make<br>handling direct requests easier for such drivers; Also for drivers that<br>expect bios, a helper function could be provided for setting up a bio<br>corresponding to a data buffer]</p>
<p>&lt;JENS: I dont understand the above, why is end_that_request_first() not<br>usable? Or _last for that matter. I must be missing something&gt;<br>&lt;SUP: What I meant here was that if the request doesn’t have a bio, then<br> end_that_request_first doesn’t modify nr_sectors or current_nr_sectors,<br> and hence can’t be used for advancing request state settings on the<br> completion of partial transfers. The driver has to modify these fields<br> directly by hand.<br> This is because end_that_request_first only iterates over the bio list,<br> and always returns 0 if there are none associated with the request.<br> _last works OK in this case, and is not a problem, as I mentioned earlier</p>
<blockquote>
</blockquote>
<p>1.3.1 Pre-built Commands</p>
<p>A request can be created with a pre-built custom command  to be sent directly<br>to the device. The cmd block in the request structure has room for filling<br>in the command bytes. (i.e rq-&gt;cmd is now 16 bytes in size, and meant for<br>command pre-building, and the type of the request is now indicated<br>through rq-&gt;flags instead of via rq-&gt;cmd)</p>
<p>The request structure flags can be set up to indicate the type of request<br>in such cases (REQ_PC: direct packet command passed to driver, REQ_BLOCK_PC:<br>packet command issued via blk_do_rq, REQ_SPECIAL: special request).</p>
<p>It can help to pre-build device commands for requests in advance.<br>Drivers can now specify a request prepare function (q-&gt;prep_rq_fn) that the<br>block layer would invoke to pre-build device commands for a given request,<br>or perform other preparatory processing for the request. This is routine is<br>called by elv_next_request(), i.e. typically just before servicing a request.<br>(The prepare function would not be called for requests that have REQ_DONTPREP<br>enabled)</p>
<p>Aside:<br>  Pre-building could possibly even be done early, i.e before placing the<br>  request on the queue, rather than construct the command on the fly in the<br>  driver while servicing the request queue when it may affect latencies in<br>  interrupt context or responsiveness in general. One way to add early<br>  pre-building would be to do it whenever we fail to merge on a request.<br>  Now REQ_NOMERGE is set in the request flags to skip this one in the future,<br>  which means that it will not change before we feed it to the device. So<br>  the pre-builder hook can be invoked there.</p>
<ol start="2">
<li>Flexible and generic but minimalist i/o structure/descriptor.</li>
</ol>
<p>2.1 Reason for a new structure and requirements addressed</p>
<p>Prior to 2.5, buffer heads were used as the unit of i/o at the generic block<br>layer, and the low level request structure was associated with a chain of<br>buffer heads for a contiguous i/o request. This led to certain inefficiencies<br>when it came to large i/o requests and readv/writev style operations, as it<br>forced such requests to be broken up into small chunks before being passed<br>on to the generic block layer, only to be merged by the i/o scheduler<br>when the underlying device was capable of handling the i/o in one shot.<br>Also, using the buffer head as an i/o structure for i/os that didn’t originate<br>from the buffer cache unnecessarily added to the weight of the descriptors<br>which were generated for each such chunk.</p>
<p>The following were some of the goals and expectations considered in the<br>redesign of the block i/o data structure in 2.5.</p>
<p>i.  Should be appropriate as a descriptor for both raw and buffered i/o  -<br>    avoid cache related fields which are irrelevant in the direct/page i/o path,<br>    or filesystem block size alignment restrictions which may not be relevant<br>    for raw i/o.<br>ii. Ability to represent high-memory buffers (which do not have a virtual<br>    address mapping in kernel address space).<br>iii.Ability to represent large i/os w/o unnecessarily breaking them up (i.e<br>    greater than PAGE_SIZE chunks in one shot)<br>iv. At the same time, ability to retain independent identity of i/os from<br>    different sources or i/o units requiring individual completion (e.g. for<br>    latency reasons)<br>v.  Ability to represent an i/o involving multiple physical memory segments<br>    (including non-page aligned page fragments, as specified via readv/writev)<br>    without unnecessarily breaking it up, if the underlying device is capable of<br>    handling it.<br>vi. Preferably should be based on a memory descriptor structure that can be<br>    passed around different types of subsystems or layers, maybe even<br>    networking, without duplication or extra copies of data/descriptor fields<br>    themselves in the process<br>vii.Ability to handle the possibility of splits/merges as the structure passes<br>    through layered drivers (lvm, md, evms), with minimal overhead.</p>
<p>The solution was to define a new structure (bio)  for the block layer,<br>instead of using the buffer head structure (bh) directly, the idea being<br>avoidance of some associated baggage and limitations. The bio structure<br>is uniformly used for all i/o at the block layer ; it forms a part of the<br>bh structure for buffered i/o, and in the case of raw/direct i/o kiobufs are<br>mapped to bio structures.</p>
<p>2.2 The bio struct</p>
<p>The bio structure uses a vector representation pointing to an array of tuples<br>of &lt;page, offset, len&gt; to describe the i/o buffer, and has various other<br>fields describing i/o parameters and state that needs to be maintained for<br>performing the i/o.</p>
<p>Notice that this representation means that a bio has no virtual address<br>mapping at all (unlike buffer heads).</p>
<p>struct bio_vec {<br>       struct page     *bv_page;<br>       unsigned short  bv_len;<br>       unsigned short  bv_offset;<br>};</p>
<p>/*</p>
<ul>
<li><p>main unit of I/O for the block layer and lower layers (ie drivers)</p>
</li>
<li><p>/<br>struct bio {</p>
<pre><code> sector_t            bi_sector;
 struct bio          *bi_next;    /* request queue link */
 struct block_device *bi_bdev;    /* target device */
 unsigned long       bi_flags;    /* status, command, etc */
 unsigned long       bi_rw;       /* low bits: r/w, high: priority */

 unsigned int    bi_vcnt;     /* how may bio_vec&#39;s */
 unsigned int    bi_idx;        /* current index into bio_vec array */

 unsigned int    bi_size;     /* total size in bytes */
 unsigned short     bi_phys_segments; /* segments after physaddr coalesce*/
 unsigned short    bi_hw_segments; /* segments after DMA remapping */
 unsigned int    bi_max;         /* max bio_vecs we can hold
                                  used as index into pool */
 struct bio_vec   *bi_io_vec;  /* the actual vec list */
 bio_end_io_t    *bi_end_io;  /* bi_end_io (bio) */
 atomic_t        bi_cnt;         /* pin count: free when it hits zero */
 void             *bi_private;
</code></pre>
<p>};</p>
</li>
</ul>
<p>With this multipage bio design:</p>
<ul>
<li>Large i/os can be sent down in one go using a bio_vec list consisting<br>of an array of &lt;page, offset, len&gt; fragments (similar to the way fragments<br>are represented in the zero-copy network code)</li>
<li>Splitting of an i/o request across multiple devices (as in the case of<br>lvm or raid) is achieved by cloning the bio (where the clone points to<br>the same bi_io_vec array, but with the index and size accordingly modified)</li>
<li>A linked list of bios is used as before for unrelated merges (*) - this<br>avoids reallocs and makes independent completions easier to handle.</li>
<li>Code that traverses the req list can find all the segments of a bio<br>by using rq_for_each_segment.  This handles the fact that a request<br>has multiple bios, each of which can have multiple segments.</li>
<li>Drivers which can’t process a large bio in one shot can use the bi_idx<br>field to keep track of the next bio_vec entry to process.<br>(e.g a 1MB bio_vec needs to be handled in max 128kB chunks for IDE)<br>[TBD: Should preferably also have a bi_voffset and bi_vlen to avoid modifying<br> bi_offset an len fields]</li>
</ul>
<p>(*) unrelated merges – a request ends up containing two or more bios that<br>    didn’t originate from the same place.</p>
<p>bi_end_io() i/o callback gets called on i/o completion of the entire bio.</p>
<p>At a lower level, drivers build a scatter gather list from the merged bios.<br>The scatter gather list is in the form of an array of &lt;page, offset, len&gt;<br>entries with their corresponding dma address mappings filled in at the<br>appropriate time. As an optimization, contiguous physical pages can be<br>covered by a single entry where <page> refers to the first page and <len><br>covers the range of pages (up to 16 contiguous pages could be covered this<br>way). There is a helper routine (blk_rq_map_sg) which drivers can use to build<br>the sg list.</p>
<p>Note: Right now the only user of bios with more than one page is ll_rw_kio,<br>which in turn means that only raw I/O uses it (direct i/o may not work<br>right now). The intent however is to enable clustering of pages etc to<br>become possible. The pagebuf abstraction layer from SGI also uses multi-page<br>bios, but that is currently not included in the stock development kernels.<br>The same is true of Andrew Morton’s work-in-progress multipage bio writeout<br>and readahead patches.</p>
<p>2.3 Changes in the Request Structure</p>
<p>The request structure is the structure that gets passed down to low level<br>drivers. The block layer make_request function builds up a request structure,<br>places it on the queue and invokes the drivers request_fn. The driver makes<br>use of block layer helper routine elv_next_request to pull the next request<br>off the queue. Control or diagnostic functions might bypass block and directly<br>invoke underlying driver entry points passing in a specially constructed<br>request structure.</p>
<p>Only some relevant fields (mainly those which changed or may be referred<br>to in some of the discussion here) are listed below, not necessarily in<br>the order in which they occur in the structure (see include/linux/blkdev.h)<br>Refer to Documentation/block/request.txt for details about all the request<br>structure fields and a quick reference about the layers which are<br>supposed to use or modify those fields.</p>
<p>struct request {<br>    struct list_head queuelist;  /* Not meant to be directly accessed by<br>                    the driver.<br>                    Used by q-&gt;elv_next_request_fn<br>                    rq-&gt;queue is gone<br>                    <em>/<br>    .<br>    .<br>    unsigned char cmd[16]; /</em> prebuilt command data block <em>/<br>    unsigned long flags;   /</em> also includes earlier rq-&gt;cmd settings <em>/<br>    .<br>    .<br>    sector_t sector; /</em> this field is now of type sector_t instead of int<br>                preparation for 64 bit sectors */<br>    .<br>    .</p>
<pre><code>/* Number of scatter-gather DMA addr+len pairs after
 * physical address coalescing is performed.
 */
unsigned short nr_phys_segments;

/* Number of scatter-gather addr+len pairs after
 * physical and DMA remapping hardware coalescing is performed.
 * This is the number of scatter-gather entries the driver
 * will actually have to deal with after DMA mapping is done.
 */
unsigned short nr_hw_segments;

/* Various sector counts */
unsigned long nr_sectors;  /* no. of sectors left: driver modifiable */
unsigned long hard_nr_sectors;  /* block internal copy of above */
unsigned int current_nr_sectors; /* no. of sectors left in the
                   current segment:driver modifiable */
unsigned long hard_cur_sectors; /* block internal copy of the above */
.
.
int tag;    /* command tag associated with request */
void *special;  /* same as before */
char *buffer;   /* valid only for low memory buffers up to
         current_nr_sectors */
.
.
struct bio *bio, *biotail;  /* bio list instead of bh */
struct request_list *rl;
</code></pre>
<p>}</p>
<p>See the rq_flag_bits definitions for an explanation of the various flags<br>available. Some bits are used by the block layer or i/o scheduler.</p>
<p>The behaviour of the various sector counts are almost the same as before,<br>except that since we have multi-segment bios, current_nr_sectors refers<br>to the numbers of sectors in the current segment being processed which could<br>be one of the many segments in the current bio (i.e i/o completion unit).<br>The nr_sectors value refers to the total number of sectors in the whole<br>request that remain to be transferred (no change). The purpose of the<br>hard_xxx values is for block to remember these counts every time it hands<br>over the request to the driver. These values are updated by block on<br>end_that_request_first, i.e. every time the driver completes a part of the<br>transfer and invokes block end*request helpers to mark this. The<br>driver should not modify these values. The block layer sets up the<br>nr_sectors and current_nr_sectors fields (based on the corresponding<br>hard_xxx values and the number of bytes transferred) and updates it on<br>every transfer that invokes end_that_request_first. It does the same for the<br>buffer, bio, bio-&gt;bi_idx fields too.</p>
<p>The buffer field is just a virtual address mapping of the current segment<br>of the i/o buffer in cases where the buffer resides in low-memory. For high<br>memory i/o, this field is not valid and must not be used by drivers.</p>
<p>Code that sets up its own request structures and passes them down to<br>a driver needs to be careful about interoperation with the block layer helper<br>functions which the driver uses. (Section 1.3)</p>
<ol start="3">
<li>Using bios</li>
</ol>
<p>3.1 Setup/Teardown</p>
<p>There are routines for managing the allocation, and reference counting, and<br>freeing of bios (bio_alloc, bio_get, bio_put).</p>
<p>This makes use of Ingo Molnar’s mempool implementation, which enables<br>subsystems like bio to maintain their own reserve memory pools for guaranteed<br>deadlock-free allocations during extreme VM load. For example, the VM<br>subsystem makes use of the block layer to writeout dirty pages in order to be<br>able to free up memory space, a case which needs careful handling. The<br>allocation logic draws from the preallocated emergency reserve in situations<br>where it cannot allocate through normal means. If the pool is empty and it<br>can wait, then it would trigger action that would help free up memory or<br>replenish the pool (without deadlocking) and wait for availability in the pool.<br>If it is in IRQ context, and hence not in a position to do this, allocation<br>could fail if the pool is empty. In general mempool always first tries to<br>perform allocation without having to wait, even if it means digging into the<br>pool as long it is not less that 50% full.</p>
<p>On a free, memory is released to the pool or directly freed depending on<br>the current availability in the pool. The mempool interface lets the<br>subsystem specify the routines to be used for normal alloc and free. In the<br>case of bio, these routines make use of the standard slab allocator.</p>
<p>The caller of bio_alloc is expected to taken certain steps to avoid<br>deadlocks, e.g. avoid trying to allocate more memory from the pool while<br>already holding memory obtained from the pool.<br>[TBD: This is a potential issue, though a rare possibility<br> in the bounce bio allocation that happens in the current code, since<br> it ends up allocating a second bio from the same pool while<br> holding the original bio ]</p>
<p>Memory allocated from the pool should be released back within a limited<br>amount of time (in the case of bio, that would be after the i/o is completed).<br>This ensures that if part of the pool has been used up, some work (in this<br>case i/o) must already be in progress and memory would be available when it<br>is over. If allocating from multiple pools in the same code path, the order<br>or hierarchy of allocation needs to be consistent, just the way one deals<br>with multiple locks.</p>
<p>The bio_alloc routine also needs to allocate the bio_vec_list (bvec_alloc())<br>for a non-clone bio. There are the 6 pools setup for different size biovecs,<br>so bio_alloc(gfp_mask, nr_iovecs) will allocate a vec_list of the<br>given size from these slabs.</p>
<p>The bio_get() routine may be used to hold an extra reference on a bio prior<br>to i/o submission, if the bio fields are likely to be accessed after the<br>i/o is issued (since the bio may otherwise get freed in case i/o completion<br>happens in the meantime).</p>
<p>The bio_clone() routine may be used to duplicate a bio, where the clone<br>shares the bio_vec_list with the original bio (i.e. both point to the<br>same bio_vec_list). This would typically be used for splitting i/o requests<br>in lvm or md.</p>
<p>3.2 Generic bio helper Routines</p>
<p>3.2.1 Traversing segments and completion units in a request</p>
<p>The macro rq_for_each_segment() should be used for traversing the bios<br>in the request list (drivers should avoid directly trying to do it<br>themselves). Using these helpers should also make it easier to cope<br>with block changes in the future.</p>
<pre><code>struct req_iterator iter;
rq_for_each_segment(bio_vec, rq, iter)
    /* bio_vec is now current segment */
</code></pre>
<p>I/O completion callbacks are per-bio rather than per-segment, so drivers<br>that traverse bio chains on completion need to keep that in mind. Drivers<br>which don’t make a distinction between segments and completion units would<br>need to be reorganized to support multi-segment bios.</p>
<p>3.2.2 Setting up DMA scatterlists</p>
<p>The blk_rq_map_sg() helper routine would be used for setting up scatter<br>gather lists from a request, so a driver need not do it on its own.</p>
<pre><code>nr_segments = blk_rq_map_sg(q, rq, scatterlist);
</code></pre>
<p>The helper routine provides a level of abstraction which makes it easier<br>to modify the internals of request to scatterlist conversion down the line<br>without breaking drivers. The blk_rq_map_sg routine takes care of several<br>things like collapsing physically contiguous segments (if QUEUE_FLAG_CLUSTER<br>is set) and correct segment accounting to avoid exceeding the limits which<br>the i/o hardware can handle, based on various queue properties.</p>
<ul>
<li>Prevents a clustered segment from crossing a 4GB mem boundary</li>
<li>Avoids building segments that would exceed the number of physical<br>memory segments that the driver can handle (phys_segments) and the<br>number that the underlying hardware can handle at once, accounting for<br>DMA remapping (hw_segments)  (i.e. IOMMU aware limits).</li>
</ul>
<p>Routines which the low level driver can use to set up the segment limits:</p>
<p>blk_queue_max_hw_segments() : Sets an upper limit of the maximum number of<br>hw data segments in a request (i.e. the maximum number of address/length<br>pairs the host adapter can actually hand to the device at once)</p>
<p>blk_queue_max_phys_segments() : Sets an upper limit on the maximum number<br>of physical data segments in a request (i.e. the largest sized scatter list<br>a driver could handle)</p>
<p>3.2.3 I/O completion</p>
<p>The existing generic block layer helper routines end_request,<br>end_that_request_first and end_that_request_last can be used for i/o<br>completion (and setting things up so the rest of the i/o or the next<br>request can be kicked of) as before. With the introduction of multi-page<br>bio support, end_that_request_first requires an additional argument indicating<br>the number of sectors completed.</p>
<p>3.2.4 Implications for drivers that do not interpret bios (don’t handle<br> multiple segments)</p>
<p>Drivers that do not interpret bios e.g those which do not handle multiple<br>segments and do not support i/o into high memory addresses (require bounce<br>buffers) and expect only virtually mapped buffers, can access the rq-&gt;buffer<br>field. As before the driver should use current_nr_sectors to determine the<br>size of remaining data in the current segment (that is the maximum it can<br>transfer in one go unless it interprets segments), and rely on the block layer<br>end_request, or end_that_request_first/last to take care of all accounting<br>and transparent mapping of the next bio segment when a segment boundary<br>is crossed on completion of a transfer. (The end<em>request</em> functions should<br>be used if only if the request has come down from block/bio path, not for<br>direct access requests which only specify rq-&gt;buffer without a valid rq-&gt;bio)</p>
<p>3.2.5 Generic request command tagging</p>
<p>3.2.5.1 Tag helpers</p>
<p>Block now offers some simple generic functionality to help support command<br>queueing (typically known as tagged command queueing), ie manage more than<br>one outstanding command on a queue at any given time.</p>
<pre><code>blk_queue_init_tags(struct request_queue *q, int depth)

Initialize internal command tagging structures for a maximum
depth of &#39;depth&#39;.

blk_queue_free_tags((struct request_queue *q)

Teardown tag info associated with the queue. This will be done
automatically by block if blk_queue_cleanup() is called on a queue
that is using tagging.
</code></pre>
<p>The above are initialization and exit management, the main helpers during<br>normal operations are:</p>
<pre><code>blk_queue_start_tag(struct request_queue *q, struct request *rq)

Start tagged operation for this request. A free tag number between
0 and &#39;depth&#39; is assigned to the request (rq-&gt;tag holds this number),
and &#39;rq&#39; is added to the internal tag management. If the maximum depth
for this queue is already achieved (or if the tag wasn&#39;t started for
some other reason), 1 is returned. Otherwise 0 is returned.

blk_queue_end_tag(struct request_queue *q, struct request *rq)

End tagged operation on this request. &#39;rq&#39; is removed from the internal
book keeping structures.
</code></pre>
<p>To minimize struct request and queue overhead, the tag helpers utilize some<br>of the same request members that are used for normal request queue management.<br>This means that a request cannot both be an active tag and be on the queue<br>list at the same time. blk_queue_start_tag() will remove the request, but<br>the driver must remember to call blk_queue_end_tag() before signalling<br>completion of the request to the block layer. This means ending tag<br>operations before calling end_that_request_last()! For an example of a user<br>of these helpers, see the IDE tagged command queueing support.</p>
<p>Certain hardware conditions may dictate a need to invalidate the block tag<br>queue. For instance, on IDE any tagged request error needs to clear both<br>the hardware and software block queue and enable the driver to sanely restart<br>all the outstanding requests. There’s a third helper to do that:</p>
<pre><code>blk_queue_invalidate_tags(struct request_queue *q)

Clear the internal block tag queue and re-add all the pending requests
to the request queue. The driver will receive them again on the
next request_fn run, just like it did the first time it encountered
them.
</code></pre>
<p>3.2.5.2 Tag info</p>
<p>Some block functions exist to query current tag status or to go from a<br>tag number to the associated request. These are, in no particular order:</p>
<pre><code>blk_queue_tagged(q)

Returns 1 if the queue &#39;q&#39; is using tagging, 0 if not.

blk_queue_tag_request(q, tag)

Returns a pointer to the request associated with tag &#39;tag&#39;.

blk_queue_tag_depth(q)

Return current queue depth.

blk_queue_tag_queue(q)

Returns 1 if the queue can accept a new queued command, 0 if we are
at the maximum depth already.

blk_queue_rq_tagged(rq)

Returns 1 if the request &#39;rq&#39; is tagged.
</code></pre>
<p>3.2.5.2 Internal structure</p>
<p>Internally, block manages tags in the blk_queue_tag structure:</p>
<pre><code>struct blk_queue_tag &#123;
    struct request **tag_index;    /* array or pointers to rq */
    unsigned long *tag_map;        /* bitmap of free tags */
    struct list_head busy_list;    /* fifo list of busy tags */
    int busy;            /* queue depth */
    int max_depth;            /* max queue depth */
&#125;;
</code></pre>
<p>Most of the above is simple and straight forward, however busy_list may need<br>a bit of explaining. Normally we don’t care too much about request ordering,<br>but in the event of any barrier requests in the tag queue we need to ensure<br>that requests are restarted in the order they were queue. This may happen<br>if the driver needs to use blk_queue_invalidate_tags().</p>
<p>3.3 I/O Submission</p>
<p>The routine submit_bio() is used to submit a single io. Higher level i/o<br>routines make use of this:</p>
<p>(a) Buffered i/o:<br>The routine submit_bh() invokes submit_bio() on a bio corresponding to the<br>bh, allocating the bio if required. ll_rw_block() uses submit_bh() as before.</p>
<p>(b) Kiobuf i/o (for raw/direct i/o):<br>The ll_rw_kio() routine breaks up the kiobuf into page sized chunks and<br>maps the array to one or more multi-page bios, issuing submit_bio() to<br>perform the i/o on each of these.</p>
<p>The embedded bh array in the kiobuf structure has been removed and no<br>preallocation of bios is done for kiobufs. [The intent is to remove the<br>blocks array as well, but it’s currently in there to kludge around direct i/o.]<br>Thus kiobuf allocation has switched back to using kmalloc rather than vmalloc.</p>
<p>Todo/Observation:</p>
<p> A single kiobuf structure is assumed to correspond to a contiguous range<br> of data, so brw_kiovec() invokes ll_rw_kio for each kiobuf in a kiovec.<br> So right now it wouldn’t work for direct i/o on non-contiguous blocks.<br> This is to be resolved.  The eventual direction is to replace kiobuf<br> by kvec’s.</p>
<p> Badari Pulavarty has a patch to implement direct i/o correctly using<br> bio and kvec.</p>
<p>(c) Page i/o:<br>Todo/Under discussion:</p>
<p> Andrew Morton’s multi-page bio patches attempt to issue multi-page<br> writeouts (and reads) from the page cache, by directly building up<br> large bios for submission completely bypassing the usage of buffer<br> heads. This work is still in progress.</p>
<p> Christoph Hellwig had some code that uses bios for page-io (rather than<br> bh). This isn’t included in bio as yet. Christoph was also working on a<br> design for representing virtual/real extents as an entity and modifying<br> some of the address space ops interfaces to utilize this abstraction rather<br> than buffer_heads. (This is somewhat along the lines of the SGI XFS pagebuf<br> abstraction, but intended to be as lightweight as possible).</p>
<p>(d) Direct access i/o:<br>Direct access requests that do not contain bios would be submitted differently<br>as discussed earlier in section 1.3.</p>
<p>Aside:</p>
<p>  Kvec i/o:</p>
<p>  Ben LaHaise’s aio code uses a slightly different structure instead<br>  of kiobufs, called a kvec_cb. This contains an array of &lt;page, offset, len&gt;<br>  tuples (very much like the networking code), together with a callback function<br>  and data pointer. This is embedded into a brw_cb structure when passed<br>  to brw_kvec_async().</p>
<p>  Now it should be possible to directly map these kvecs to a bio. Just as while<br>  cloning, in this case rather than PRE_BUILT bio_vecs, we set the bi_io_vec<br>  array pointer to point to the veclet array in kvecs.</p>
<p>  TBD: In order for this to work, some changes are needed in the way multi-page<br>  bios are handled today. The values of the tuples in such a vector passed in<br>  from higher level code should not be modified by the block layer in the course<br>  of its request processing, since that would make it hard for the higher layer<br>  to continue to use the vector descriptor (kvec) after i/o completes. Instead,<br>  all such transient state should either be maintained in the request structure,<br>  and passed on in some way to the endio completion routine.</p>
<ol start="4">
<li>The I/O scheduler<br>I/O scheduler, a.k.a. elevator, is implemented in two layers.  Generic dispatch<br>queue and specific I/O schedulers.  Unless stated otherwise, elevator is used<br>to refer to both parts and I/O scheduler to specific I/O schedulers.</li>
</ol>
<p>Block layer implements generic dispatch queue in block/*.c.<br>The generic dispatch queue is responsible for properly ordering barrier<br>requests, requeueing, handling non-fs requests and all other subtleties.</p>
<p>Specific I/O schedulers are responsible for ordering normal filesystem<br>requests.  They can also choose to delay certain requests to improve<br>throughput or whatever purpose.  As the plural form indicates, there are<br>multiple I/O schedulers.  They can be built as modules but at least one should<br>be built inside the kernel.  Each queue can choose different one and can also<br>change to another one dynamically.</p>
<p>A block layer call to the i/o scheduler follows the convention elv_xxx(). This<br>calls elevator_xxx_fn in the elevator switch (block/elevator.c). Oh, xxx<br>and xxx might not match exactly, but use your imagination. If an elevator<br>doesn’t implement a function, the switch does nothing or some minimal house<br>keeping work.</p>
<p>4.1. I/O scheduler API</p>
<p>The functions an elevator may implement are: (* are mandatory)<br>elevator_merge_fn        called to query requests for merge with a bio</p>
<p>elevator_merge_req_fn        called when two requests get merged. the one<br>                which gets merged into the other one will be<br>                never seen by I/O scheduler again. IOW, after<br>                being merged, the request is gone.</p>
<p>elevator_merged_fn        called when a request in the scheduler has been<br>                involved in a merge. It is used in the deadline<br>                scheduler for example, to reposition the request<br>                if its sorting order has changed.</p>
<p>elevator_allow_merge_fn        called whenever the block layer determines<br>                that a bio can be merged into an existing<br>                request safely. The io scheduler may still<br>                want to stop a merge at this point if it<br>                results in some sort of conflict internally,<br>                this hook allows it to do that.</p>
<p>elevator_dispatch_fn*        fills the dispatch queue with ready requests.<br>                I/O schedulers are free to postpone requests by<br>                not filling the dispatch queue unless @force<br>                is non-zero.  Once dispatched, I/O schedulers<br>                are not allowed to manipulate the requests -<br>                they belong to generic dispatch queue.</p>
<p>elevator_add_req_fn*        called to add a new request into the scheduler</p>
<p>elevator_former_req_fn<br>elevator_latter_req_fn        These return the request before or after the<br>                one specified in disk sort order. Used by the<br>                block layer to find merge possibilities.</p>
<p>elevator_completed_req_fn    called when a request is completed.</p>
<p>elevator_may_queue_fn        returns true if the scheduler wants to allow the<br>                current context to queue a new request even if<br>                it is over the queue limit. This must be used<br>                very carefully!!</p>
<p>elevator_set_req_fn<br>elevator_put_req_fn        Must be used to allocate and free any elevator<br>                specific storage for a request.</p>
<p>elevator_activate_req_fn    Called when device driver first sees a request.<br>                I/O schedulers can use this callback to<br>                determine when actual execution of a request<br>                starts.<br>elevator_deactivate_req_fn    Called when device driver decides to delay<br>                a request by requeueing it.</p>
<p>elevator_init_fn*<br>elevator_exit_fn        Allocate and free any elevator specific storage<br>                for a queue.</p>
<p>4.2 Request flows seen by I/O schedulers<br>All requests seen by I/O schedulers strictly follow one of the following three<br>flows.</p>
<p> set_req_fn -&gt;</p>
<p> i.   add_req_fn -&gt; (merged_fn -&gt;)* -&gt; dispatch_fn -&gt; activate_req_fn -&gt;<br>      (deactivate_req_fn -&gt; activate_req_fn -&gt;)* -&gt; completed_req_fn<br> ii.  add_req_fn -&gt; (merged_fn -&gt;)* -&gt; merge_req_fn<br> iii. [none]</p>
<p> -&gt; put_req_fn</p>
<p>4.3 I/O scheduler implementation<br>The generic i/o scheduler algorithm attempts to sort/merge/batch requests for<br>optimal disk scan and request servicing performance (based on generic<br>principles and device capabilities), optimized for:<br>i.   improved throughput<br>ii.  improved latency<br>iii. better utilization of h/w &amp; CPU time</p>
<p>Characteristics:</p>
<p>i. Binary tree<br>AS and deadline i/o schedulers use red black binary trees for disk position<br>sorting and searching, and a fifo linked list for time-based searching. This<br>gives good scalability and good availability of information. Requests are<br>almost always dispatched in disk sort order, so a cache is kept of the next<br>request in sort order to prevent binary tree lookups.</p>
<p>This arrangement is not a generic block layer characteristic however, so<br>elevators may implement queues as they please.</p>
<p>ii. Merge hash<br>AS and deadline use a hash table indexed by the last sector of a request. This<br>enables merging code to quickly look up “back merge” candidates, even when<br>multiple I/O streams are being performed at once on one disk.</p>
<p>“Front merges”, a new request being merged at the front of an existing request,<br>are far less common than “back merges” due to the nature of most I/O patterns.<br>Front merges are handled by the binary trees in AS and deadline schedulers.</p>
<p>iii. Plugging the queue to batch requests in anticipation of opportunities for<br>     merge/sort optimizations</p>
<p>Plugging is an approach that the current i/o scheduling algorithm resorts to so<br>that it collects up enough requests in the queue to be able to take<br>advantage of the sorting/merging logic in the elevator. If the<br>queue is empty when a request comes in, then it plugs the request queue<br>(sort of like plugging the bath tub of a vessel to get fluid to build up)<br>till it fills up with a few more requests, before starting to service<br>the requests. This provides an opportunity to merge/sort the requests before<br>passing them down to the device. There are various conditions when the queue is<br>unplugged (to open up the flow again), either through a scheduled task or<br>could be on demand. For example wait_on_buffer sets the unplugging going<br>through sync_buffer() running blk_run_address_space(mapping). Or the caller<br>can do it explicity through blk_unplug(bdev). So in the read case,<br>the queue gets explicitly unplugged as part of waiting for completion on that<br>buffer. For page driven IO, the address space -&gt;sync_page() takes care of<br>doing the blk_run_address_space().</p>
<p>Aside:<br>  This is kind of controversial territory, as it’s not clear if plugging is<br>  always the right thing to do. Devices typically have their own queues,<br>  and allowing a big queue to build up in software, while letting the device be<br>  idle for a while may not always make sense. The trick is to handle the fine<br>  balance between when to plug and when to open up. Also now that we have<br>  multi-page bios being queued in one shot, we may not need to wait to merge<br>  a big request from the broken up pieces coming by.</p>
<p>4.4 I/O contexts<br>I/O contexts provide a dynamically allocated per process data area. They may<br>be used in I/O schedulers, and in the block layer (could be used for IO statis,<br>priorities for example). See *io_context in block/ll_rw_blk.c, and as-iosched.c<br>for an example of usage in an i/o scheduler.</p>
<ol start="5">
<li>Scalability related changes</li>
</ol>
<p>5.1 Granular Locking: io_request_lock replaced by a per-queue lock</p>
<p>The global io_request_lock has been removed as of 2.5, to avoid<br>the scalability bottleneck it was causing, and has been replaced by more<br>granular locking. The request queue structure has a pointer to the<br>lock to be used for that queue. As a result, locking can now be<br>per-queue, with a provision for sharing a lock across queues if<br>necessary (e.g the scsi layer sets the queue lock pointers to the<br>corresponding adapter lock, which results in a per host locking<br>granularity). The locking semantics are the same, i.e. locking is<br>still imposed by the block layer, grabbing the lock before<br>request_fn execution which it means that lots of older drivers<br>should still be SMP safe. Drivers are free to drop the queue<br>lock themselves, if required. Drivers that explicitly used the<br>io_request_lock for serialization need to be modified accordingly.<br>Usually it’s as easy as adding a global lock:</p>
<pre><code>static DEFINE_SPINLOCK(my_driver_lock);
</code></pre>
<p>and passing the address to that lock to blk_init_queue().</p>
<p>5.2 64 bit sector numbers (sector_t prepares for 64 bit support)</p>
<p>The sector number used in the bio structure has been changed to sector_t,<br>which could be defined as 64 bit in preparation for 64 bit sector support.</p>
<ol start="6">
<li>Other Changes/Implications</li>
</ol>
<p>6.1 Partition re-mapping handled by the generic block layer</p>
<p>In 2.5 some of the gendisk/partition related code has been reorganized.<br>Now the generic block layer performs partition-remapping early and thus<br>provides drivers with a sector number relative to whole device, rather than<br>having to take partition number into account in order to arrive at the true<br>sector number. The routine blk_partition_remap() is invoked by<br>generic_make_request even before invoking the queue specific make_request_fn,<br>so the i/o scheduler also gets to operate on whole disk sector numbers. This<br>should typically not require changes to block drivers, it just never gets<br>to invoke its own partition sector offset calculations since all bios<br>sent are offset from the beginning of the device.</p>
<ol start="7">
<li>A Few Tips on Migration of older drivers</li>
</ol>
<p>Old-style drivers that just use CURRENT and ignores clustered requests,<br>may not need much change.  The generic layer will automatically handle<br>clustered requests, multi-page bios, etc for the driver.</p>
<p>For a low performance driver or hardware that is PIO driven or just doesn’t<br>support scatter-gather changes should be minimal too.</p>
<p>The following are some points to keep in mind when converting old drivers<br>to bio.</p>
<p>Drivers should use elv_next_request to pick up requests and are no longer<br>supposed to handle looping directly over the request list.<br>(struct request-&gt;queue has been removed)</p>
<p>Now end_that_request_first takes an additional number_of_sectors argument.<br>It used to handle always just the first buffer_head in a request, now<br>it will loop and handle as many sectors (on a bio-segment granularity)<br>as specified.</p>
<p>Now bh-&gt;b_end_io is replaced by bio-&gt;bi_end_io, but most of the time the<br>right thing to use is bio_endio(bio, uptodate) instead.</p>
<p>If the driver is dropping the io_request_lock from its request_fn strategy,<br>then it just needs to replace that with q-&gt;queue_lock instead.</p>
<p>As described in Sec 1.1, drivers can set max sector size, max segment size<br>etc per queue now. Drivers that used to define their own merge functions i<br>to handle things like this can now just use the blk_queue_* functions at<br>blk_init_queue time.</p>
<p>Drivers no longer have to map a {partition, sector offset} into the<br>correct absolute location anymore, this is done by the block layer, so<br>where a driver received a request ala this before:</p>
<pre><code>rq-&gt;rq_dev = mk_kdev(3, 5);    /* /dev/hda5 */
rq-&gt;sector = 0;            /* first sector on hda5 */
</code></pre>
<p>  it will now see</p>
<pre><code>rq-&gt;rq_dev = mk_kdev(3, 0);    /* /dev/hda */
rq-&gt;sector = 123128;        /* offset from start of disk */
</code></pre>
<p>As mentioned, there is no virtual mapping of a bio. For DMA, this is<br>not a problem as the driver probably never will need a virtual mapping.<br>Instead it needs a bus mapping (dma_map_page for a single segment or<br>use dma_map_sg for scatter gather) to be able to ship it to the driver. For<br>PIO drivers (or drivers that need to revert to PIO transfer once in a<br>while (IDE for example)), where the CPU is doing the actual data<br>transfer a virtual mapping is needed. If the driver supports highmem I/O,<br>(Sec 1.1, (ii) ) it needs to use __bio_kmap_atomic and bio_kmap_irq to<br>temporarily map a bio into the virtual address space.</p>
<ol start="8">
<li>Prior/Related/Impacted patches</li>
</ol>
<p>8.1. Earlier kiobuf patches (sct/axboe/chait/hch/mkp)</p>
<ul>
<li>orig kiobuf &amp; raw i/o patches (now in 2.4 tree)</li>
<li>direct kiobuf based i/o to devices (no intermediate bh’s)</li>
<li>page i/o using kiobuf</li>
<li>kiobuf splitting for lvm (mkp)</li>
<li>elevator support for kiobuf request merging (axboe)</li>
</ul>
<p>8.2. Zero-copy networking (Dave Miller)<br>8.3. SGI XFS - pagebuf patches - use of kiobufs<br>8.4. Multi-page pioent patch for bio (Christoph Hellwig)<br>8.5. Direct i/o implementation (Andrea Arcangeli) since 2.4.10-pre11<br>8.6. Async i/o implementation patch (Ben LaHaise)<br>8.7. EVMS layering design (IBM EVMS team)<br>8.8. Larger page cache size patch (Ben LaHaise) and<br>     Large page size (Daniel Phillips)<br>    =&gt; larger contiguous physical memory buffers<br>8.9. VM reservations patch (Ben LaHaise)<br>8.10. Write clustering patches ? (Marcelo/Quintela/Riel ?)<br>8.11. Block device in page cache patch (Andrea Archangeli) - now in 2.4.10+<br>8.12. Multiple block-size transfers for faster raw i/o (Shailabh Nagar,<br>      Badari)<br>8.13  Priority based i/o scheduler - prepatches (Arjan van de Ven)<br>8.14  IDE Taskfile i/o patch (Andre Hedrick)<br>8.15  Multi-page writeout and readahead patches (Andrew Morton)<br>8.16  Direct i/o patches for 2.5 using kvec and bio (Badari Pulavarthy)</p>
<ol start="9">
<li>Other References:</li>
</ol>
<p>9.1 The Splice I/O Model - Larry McVoy (and subsequent discussions on lkml,<br>and Linus’ comments - Jan 2001)<br>9.2 Discussions about kiobuf and bh design on lkml between sct, linus, alan<br>et al - Feb-March 2001 (many of the initial thoughts that led to bio were<br>brought up in this discussion thread)<br>9.3 Discussions on mempool on lkml - Dec 2001.</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_biodoc/" title="Kernel-3.10.0-957.el7_biodoc" target="_blank" rel="external">http://www.oostech.com/2021/03/13/Kernel-3.10.0-957.el7_biodoc/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">oosTech by Sam Lee</span><small class="ml-1x"></small></a></h3>
        <div></div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    

  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2021/03/13/Kernel-3.10.0-957.el7_booting-without-of/" title="Kernel-3.10.0-957.el7_booting-without-of"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2021/03/13/Kernel-3.10.0-957.el7_xfs-delayed-logging-design/" title="Kernel-3.10.0-957.el7_xfs-delayed-logging-design"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">
        <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,wechat"></div>
    
  </div>
  </div>
    <div class="container" align="left">
    <div class="post-gallery" itemscope itemtype="http://schema.org/ImageGallery">
      <img src="/images/wechatp.png" itemprop="contentUrl">
      <span>欢迎关注公众号</span>
    </div>
    </div>
</nav>

  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>您能有收获就是我们最大的动力</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">不管多少都是对分享的肯定</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码捐赠</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">不管多少都是对分享的肯定</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码捐赠</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>




</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://google.com/" target="_blank" title="Google" data-toggle=tooltip data-placement=top><i class="icon icon-google"></i></a></li>
        
        <li><a href="https://twitter.com/" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
	<a target="_blank" rel="noopener" href="https://beian.miit.gov.cn" text-align: center >粤ICP备2021024811号</a>
        </div>
<!--    
    <div>
     <img src="/images/logo.png" width="140" height="140">
    </div>
-->
    </div>


        <!-- 不蒜子统计    //busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js -->
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <!--<span id="busuanzi_container_site_pv" style="display:inline;">-->本站总访问量<span id="busuanzi_value_site_pv" style="display:inline;></span>次</span>
        <span class="post-meta-divider">|</span>
  
</footer>

  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>







   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>











</body>
</html>